{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement distutils (from versions: none)\n",
      "ERROR: No matching distribution found for distutils\n"
     ]
    }
   ],
   "source": [
    "pip install distutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark=SparkSession.builder.appName('Employee_Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Employee, department tables into a DataFrame\n",
    "employee_df=spark.read.csv('Datasets/Employee.csv', header=True, inferSchema=True)\n",
    "department_df=spark.read.csv('Datasets/Department.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "|         1|     John|     Doe|         101| 60000|  1/15/2022|\n",
      "|         2|    Alice|   Smith|         102| 65000|  2/20/2022|\n",
      "|         3|  Michael| Johnson|         101| 58000|  3/10/2022|\n",
      "|         4|    Emily|   Brown|         103| 70000|   4/5/2022|\n",
      "|         5|    David|     Lee|         102| 62000|  5/12/2022|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+--------------+---------+\n",
      "|DepartmentID|DepartmentName|ManagerID|\n",
      "+------------+--------------+---------+\n",
      "|         101|         Sales|        1|\n",
      "|         102|     Marketing|        2|\n",
      "|         103|       Finance|        4|\n",
      "|         104|            HR|     NULL|\n",
      "|         105|    Operations|        7|\n",
      "+------------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View top rows from tables\n",
    "employee_df.show(5)\n",
    "department_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "employee_df.createOrReplaceTempView(\"employee_table\")\n",
    "department_df.createOrReplaceTempView(\"department_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------------+------+-----------+---------------+\n",
      "|EmployeeID|FirstName|  LastName|DepartmentID|Salary|JoiningDate|duplicate_count|\n",
      "+----------+---------+----------+------------+------+-----------+---------------+\n",
      "|         3|  Michael|   Johnson|         101| 58000|       NULL|              2|\n",
      "|        22|    Megan|     Scott|         101| 63000| 10/10/2023|              3|\n",
      "|        19|    Kevin|     White|         102| 68000|  7/25/2023|              3|\n",
      "|         9|   Robert|    Taylor|         104| 68000|   9/5/2022|              3|\n",
      "|        34|   Hannah|   Collins|         101| 66000| 10/10/2024|              3|\n",
      "|        28|   Alexis|    Turner|         103| 76000|  4/10/2024|              3|\n",
      "|        23|  Brandon|     Green|         102| 69000| 11/15/2023|              3|\n",
      "|        38| Danielle|    Howard|         101| 67000|       NULL|              2|\n",
      "|        44|    Sarah|    Barnes|         103| 80000|  8/30/2025|              3|\n",
      "|        30|   Olivia|  Campbell|         101| 65000|  6/20/2024|              3|\n",
      "|        15|  Matthew|    Wilson|         102|  NULL|       NULL|              2|\n",
      "|        27|   Samuel|   Roberts|         102| 70000|   3/5/2024|              3|\n",
      "|        51|     NULL|    Parker|         105| 60000|  1/15/2022|              2|\n",
      "|        36| Samantha|   Sanchez|         103| 78000| 12/20/2024|              3|\n",
      "|        46|  Rebecca|      Ross|         101| 69000| 10/10/2025|              3|\n",
      "|        43|  Timothy|Richardson|         102| 74000|       NULL|              3|\n",
      "|        16|   Amanda|  Anderson|         103| 73000|  4/10/2023|              3|\n",
      "|        41|  Patrick|    Bailey|         104| 76000|  5/15/2025|              3|\n",
      "|        24|  Tiffany|     Perez|         103| 75000| 12/20/2023|              3|\n",
      "|        42|    Kayla|    Murphy|         101|  NULL|  6/20/2025|              3|\n",
      "+----------+---------+----------+------------+------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate records\n",
    "duplicate_records_df = spark.sql(\"\"\"\n",
    "    SELECT EmployeeID, FirstName, LastName, DepartmentID,Salary, JoiningDate, COUNT(*) AS duplicate_count\n",
    "    FROM employee_table\n",
    "    GROUP BY EmployeeID, FirstName, LastName, DepartmentID,Salary, JoiningDate\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Show the duplicate records\n",
    "duplicate_records_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates \n",
    "cleaned_employee_df = employee_df.dropDuplicates(['EmployeeID','FirstName', 'LastName', 'DepartmentID', 'JoiningDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-----------+---------------+\n",
      "|FirstName|LastName|DepartmentID|JoiningDate|duplicate_count|\n",
      "+---------+--------+------------+-----------+---------------+\n",
      "|  William|   Kelly|         102|   3/5/2025|              3|\n",
      "|  Zachary|  Parker|         102|  7/25/2024|              3|\n",
      "+---------+--------+------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the cleaned DataFrame as a temporary view\n",
    "cleaned_employee_df.createOrReplaceTempView(\"cleaned_employee_view\")\n",
    "\n",
    "# Check for duplicates in the cleaned DataFrame\n",
    "duplicates_still_exist_df = spark.sql(\"\"\"\n",
    "    SELECT FirstName, LastName, DepartmentID, JoiningDate, COUNT(*) AS duplicate_count\n",
    "    FROM cleaned_employee_view\n",
    "    GROUP BY FirstName, LastName, DepartmentID, JoiningDate\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicates_still_exist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have duplicates with the firstNames William and Zachary, so check which columns have duplicate values for these two employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "|        39|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|        89|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|       139|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|        31|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "|        81|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "|       131|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame\n",
    "duplicates_still_exist_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM cleaned_employee_view\n",
    "    Where FirstName LIKE '%William%' OR FirstName LIKE '%Zachary%'\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicates_still_exist_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see these employees have different employee ids, we will keep the row with lowest employeeID and remove the remaining rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "|        49|     Adam|  Cooper|         104| 78000|  1/25/2026|\n",
      "|        28|   Alexis|  Turner|         103| 76000|  4/10/2024|\n",
      "|         2|    Alice|   Smith|         102| 65000|  2/20/2022|\n",
      "|        16|   Amanda|Anderson|         103| 73000|  4/10/2023|\n",
      "|        33|   Andrew| Edwards|         104| 74000|   9/5/2024|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification partitioned by FirstName and LastName and ordered by EmployeeID\n",
    "window_spec = Window.partitionBy(\"FirstName\", \"LastName\").orderBy(\"EmployeeID\")\n",
    "\n",
    "# Add a row number column to each row based on the window specification\n",
    "cleaned_employee_df = cleaned_employee_df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Keep only the rows with row_number = 1, which corresponds to the lowest EmployeeID for each unique FirstName and LastName combination\n",
    "deduplicated_employee_df = cleaned_employee_df.filter(\"row_number = 1\").drop(\"row_number\")\n",
    "\n",
    "# Show the deduplicated DataFrame\n",
    "deduplicated_employee_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the cleaned DataFrame as a temporary view\n",
    "deduplicated_employee_df.createOrReplaceTempView(\"deduplicated_employee_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+------+-----------+---------------+\n",
      "|FirstName|LastName|DepartmentID|Salary|JoiningDate|duplicate_count|\n",
      "+---------+--------+------------+------+-----------+---------------+\n",
      "+---------+--------+------------+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame where FirstName contains 'William' or 'Zachary'\n",
    "duplicates_check_df = spark.sql(\"\"\"\n",
    "    SELECT FirstName, LastName, DepartmentID, Salary, JoiningDate, COUNT(*) AS duplicate_count\n",
    "    FROM deduplicated_employee_view\n",
    "    GROUP BY FirstName, LastName, DepartmentID, Salary, JoiningDate\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicates_check_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+------------+------+-----------+\n",
      "|EmployeeID|  FirstName| LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+-----------+---------+------------+------+-----------+\n",
      "|        49|       Adam|   Cooper|         104| 78000|  1/25/2026|\n",
      "|        28|     Alexis|   Turner|         103| 76000|  4/10/2024|\n",
      "|         2|      Alice|    Smith|         102| 65000|  2/20/2022|\n",
      "|        16|     Amanda| Anderson|         103| 73000|  4/10/2023|\n",
      "|        33|     Andrew|  Edwards|         104| 74000|   9/5/2024|\n",
      "|        14|     Ashley|    Lopez|         101| 61000|  2/28/2023|\n",
      "|        29|   Benjamin| Phillips|         104| 73000|  5/15/2024|\n",
      "|        23|    Brandon|    Green|         102| 69000| 11/15/2023|\n",
      "|        17|      Brian|    Moore|         104| 70000|  5/15/2023|\n",
      "|        13|Christopher|Hernandez|         104|  NULL|  1/25/2023|\n",
      "|        11|     Daniel| Anderson|         102| 66000| 11/15/2022|\n",
      "|        38|   Danielle|   Howard|         101| 67000|  2/28/2025|\n",
      "|         5|      David|      Lee|         102| 62000|  5/12/2022|\n",
      "|        26|  Elizabeth|   Rivera|         101| 64000|  2/28/2024|\n",
      "|         4|      Emily|    Brown|         103| 70000|   4/5/2022|\n",
      "|        50|      Emily|   Hughes|         101| 70000|  2/28/2026|\n",
      "|        45|       Eric|    Price|         104| 77000|   9/5/2025|\n",
      "|        34|     Hannah|  Collins|         101| 66000| 10/10/2024|\n",
      "|         7|      James|    Jones|         102| 63000|  7/24/2022|\n",
      "|        47|      Jason|     Long|         102| 75000| 11/15/2025|\n",
      "+----------+-----------+---------+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame where FirstName contains 'William' or 'Zachary'\n",
    "deduplicated_employee_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM deduplicated_employee_view\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "deduplicated_employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|EmployeeID|  FirstName|  LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|        13|Christopher| Hernandez|         104|  NULL|  1/25/2023|\n",
      "|        42|      Kayla|    Murphy|         101|  NULL|  6/20/2025|\n",
      "|        48|      Laura|    Brooks|         103| 81000|       NULL|\n",
      "|        52|     Lauren|      NULL|         101| 65000|  2/20/2022|\n",
      "|        15|    Matthew|    Wilson|         102| 67000|       NULL|\n",
      "|        51|       NULL|    Parker|         105| 60000|  1/15/2022|\n",
      "|        43|    Timothy|Richardson|         102| 74000|       NULL|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame where FirstName contains 'William' or 'Zachary'\n",
    "null_employee_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM deduplicated_employee_view\n",
    "    Where EmployeeID='NULL' or FirstName ='NULL' or LastName ='NULL' or DepartmentID='NULL' or Salary='NULL' or JoiningDate ='NULL'\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "null_employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m avg,when\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Replace \"NULL\" with the average salary\u001b[39;00m\n\u001b[0;32m      4\u001b[0m clean_employee_df \u001b[38;5;241m=\u001b[39m deduplicated_employee_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m----> 5\u001b[0m                     when(\u001b[43mcol\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNULL\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_salary)\u001b[38;5;241m.\u001b[39motherwise(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Show the DataFrame with \"NULL\" values replaced by the average salary\u001b[39;00m\n\u001b[0;32m      8\u001b[0m clean_employee_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,when\n",
    "\n",
    "# Replace \"NULL\" with the average salary\n",
    "clean_employee_df = deduplicated_employee_df.withColumn(\"Salary\", \n",
    "                    when(col(\"Salary\") == \"NULL\", average_salary).otherwise(col(\"Salary\")))\n",
    "\n",
    "# Show the DataFrame with \"NULL\" values replaced by the average salary\n",
    "clean_employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|EmployeeID|  FirstName|  LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|        13|Christopher| Hernandez|         104|  NULL|  1/25/2023|\n",
      "|        42|      Kayla|    Murphy|         101|  NULL|  6/20/2025|\n",
      "|        48|      Laura|    Brooks|         103| 81000|       NULL|\n",
      "|        15|    Matthew|    Wilson|         102| 67000|       NULL|\n",
      "|        51|       NULL|    Parker|         105| 60000|  1/15/2022|\n",
      "|        43|    Timothy|Richardson|         102| 74000|       NULL|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Compute the average salary\n",
    "average_salary = deduplicated_employee_df.select(avg(col(\"Salary\"))).collect()[0][0]\n",
    "\n",
    "# Fill NULL values in the DataFrame with the average salary\n",
    "deduplicated_employee_df = deduplicated_employee_df.fillna(average_salary, subset=[\"Salary\"])\n",
    "\n",
    "# Show the DataFrame with NULL values replaced by the average salary\n",
    "deduplicated_employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
