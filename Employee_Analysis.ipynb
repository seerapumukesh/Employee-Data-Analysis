{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Data Analysis Project\n",
    "\n",
    "**Introduction:**\n",
    "This project aims to analyze employee data using PySpark, a powerful analytics engine for big data processing. The dataset consists of information about employees, including their names, departments, salaries, and joining dates. The project involves data cleaning, deduplication, analysis, and visualization to gain insights into employee demographics and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, avg, when\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('Employee_Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Employee and Department tables into DataFrames\n",
    "employee_df = spark.read.csv('Datasets/Employee.csv', header=True, inferSchema=True)\n",
    "department_df = spark.read.csv('Datasets/Department.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "|         1|     John|     Doe|         101| 60000|  1/15/2022|\n",
      "|         2|    Alice|   Smith|         102| 65000|  2/20/2022|\n",
      "|         3|  Michael| Johnson|         101| 58000|  3/10/2022|\n",
      "|         4|    Emily|   Brown|         103| 70000|   4/5/2022|\n",
      "|         5|    David|     Lee|         102| 62000|  5/12/2022|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+--------------+---------+\n",
      "|DepartmentID|DepartmentName|ManagerID|\n",
      "+------------+--------------+---------+\n",
      "|         101|         Sales|        1|\n",
      "|         102|     Marketing|        2|\n",
      "|         103|       Finance|        4|\n",
      "|         104|            HR|     NULL|\n",
      "|         105|    Operations|        7|\n",
      "+------------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View top rows from tables\n",
    "employee_df.show(5)\n",
    "department_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrames as temporary views\n",
    "employee_df.createOrReplaceTempView(\"employee_table\")\n",
    "department_df.createOrReplaceTempView(\"department_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The data preprocessing steps include handling duplicates, missing values, and standardizing data.\n",
    "\n",
    "1. **Handling Duplicates**: \n",
    "    - Duplicates in the employee table are checked and removed.\n",
    "2. **Handling Missing Values**: \n",
    "    - Missing values in the DataFrame are replaced or dropped.\n",
    "3. **Standardizing Data**: \n",
    "    - String columns are converted to lowercase with the first letter capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+------------+------+-----------+---------------+\n",
      "|EmployeeID|FirstName|  LastName|DepartmentID|Salary|JoiningDate|duplicate_count|\n",
      "+----------+---------+----------+------------+------+-----------+---------------+\n",
      "|         3|  Michael|   Johnson|         101| 58000|       NULL|              2|\n",
      "|        22|    Megan|     Scott|         101| 63000| 10/10/2023|              3|\n",
      "|        19|    Kevin|     White|         102| 68000|  7/25/2023|              3|\n",
      "|         9|   Robert|    Taylor|         104| 68000|   9/5/2022|              3|\n",
      "|        34|   Hannah|   Collins|         101| 66000| 10/10/2024|              3|\n",
      "|        28|   Alexis|    Turner|         103| 76000|  4/10/2024|              3|\n",
      "|        23|  Brandon|     Green|         102| 69000| 11/15/2023|              3|\n",
      "|        38| Danielle|    Howard|         101| 67000|       NULL|              2|\n",
      "|        44|    Sarah|    Barnes|         103| 80000|  8/30/2025|              3|\n",
      "|        30|   Olivia|  Campbell|         101| 65000|  6/20/2024|              3|\n",
      "|        15|  Matthew|    Wilson|         102|  NULL|       NULL|              2|\n",
      "|        27|   Samuel|   Roberts|         102| 70000|   3/5/2024|              3|\n",
      "|        51|     NULL|    Parker|         105| 60000|  1/15/2022|              2|\n",
      "|        36| Samantha|   Sanchez|         103| 78000| 12/20/2024|              3|\n",
      "|        46|  Rebecca|      Ross|         101| 69000| 10/10/2025|              3|\n",
      "|        43|  Timothy|Richardson|         102| 74000|       NULL|              3|\n",
      "|        16|   Amanda|  Anderson|         103| 73000|  4/10/2023|              3|\n",
      "|        41|  Patrick|    Bailey|         104| 76000|  5/15/2025|              3|\n",
      "|        24|  Tiffany|     Perez|         103| 75000| 12/20/2023|              3|\n",
      "|        42|    Kayla|    Murphy|         101|  NULL|  6/20/2025|              3|\n",
      "+----------+---------+----------+------------+------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate records in the employee table\n",
    "duplicate_records_df = spark.sql(\"\"\"\n",
    "    SELECT EmployeeID, FirstName, LastName, DepartmentID, Salary, JoiningDate, COUNT(*) AS duplicate_count\n",
    "    FROM employee_table\n",
    "    GROUP BY EmployeeID, FirstName, LastName, DepartmentID, Salary, JoiningDate\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Show the duplicate records\n",
    "duplicate_records_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 48\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = duplicate_records_df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"Number of Rows:\", row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the employee DataFrame\n",
    "cleaned_employee_df = employee_df.dropDuplicates(['EmployeeID', 'FirstName', 'LastName', 'DepartmentID', 'JoiningDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the cleaned DataFrame as a temporary view\n",
    "cleaned_employee_df.createOrReplaceTempView(\"cleaned_employee_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-----------+---------------+\n",
      "|FirstName|LastName|DepartmentID|JoiningDate|duplicate_count|\n",
      "+---------+--------+------------+-----------+---------------+\n",
      "|  William|   Kelly|         102|   3/5/2025|              3|\n",
      "|  Zachary|  Parker|         102|  7/25/2024|              3|\n",
      "+---------+--------+------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame\n",
    "duplicates_still_exist_df = spark.sql(\"\"\"\n",
    "    SELECT FirstName, LastName, DepartmentID, JoiningDate, COUNT(*) AS duplicate_count\n",
    "    FROM cleaned_employee_view\n",
    "    GROUP BY FirstName, LastName, DepartmentID, JoiningDate\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicates_still_exist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "|        39|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|        89|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|       139|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|        31|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "|        81|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "|       131|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We still have duplicates with the firstNames William and Zachary, so check which columns have duplicate values for these two employees\n",
    "\n",
    "# Check for duplicates in the cleaned DataFrame\n",
    "duplicates_still_exist_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM cleaned_employee_view\n",
    "    Where FirstName LIKE '%William%' OR FirstName LIKE '%Zachary%'\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicates_still_exist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|EmployeeID|  FirstName|  LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|        49|       Adam|    Cooper|         104| 78000|  1/25/2026|\n",
      "|        28|     Alexis|    Turner|         103| 76000|  4/10/2024|\n",
      "|         2|      Alice|     Smith|         102| 65000|  2/20/2022|\n",
      "|        16|     Amanda|  Anderson|         103| 73000|  4/10/2023|\n",
      "|        33|     Andrew|   Edwards|         104| 74000|   9/5/2024|\n",
      "|        14|     Ashley|     Lopez|         101| 61000|  2/28/2023|\n",
      "|        29|   Benjamin|  Phillips|         104| 73000|  5/15/2024|\n",
      "|        23|    Brandon|     Green|         102| 69000| 11/15/2023|\n",
      "|        17|      Brian|     Moore|         104| 70000|  5/15/2023|\n",
      "|        13|Christopher| Hernandez|         104|  NULL|  1/25/2023|\n",
      "|        11|     Daniel|  Anderson|         102| 66000| 11/15/2022|\n",
      "|        38|   Danielle|    Howard|         101| 67000|  2/28/2025|\n",
      "|         5|      David|       Lee|         102| 62000|  5/12/2022|\n",
      "|        26|  Elizabeth|    Rivera|         101| 64000|  2/28/2024|\n",
      "|         4|      Emily|     Brown|         103| 70000|   4/5/2022|\n",
      "|        50|      Emily|    Hughes|         101| 70000|  2/28/2026|\n",
      "|        45|       Eric|     Price|         104| 77000|   9/5/2025|\n",
      "|        34|     Hannah|   Collins|         101| 66000| 10/10/2024|\n",
      "|         7|      James|     Jones|         102| 63000|  7/24/2022|\n",
      "|        47|      Jason|      Long|         102| 75000| 11/15/2025|\n",
      "|         8|   Jennifer|     Davis|         103| 71000|  8/30/2022|\n",
      "|        10|    Jessica|  Martinez|         101| 60000| 10/10/2022|\n",
      "|         1|       John|       Doe|         101| 60000|  1/15/2022|\n",
      "|        35|     Joshua|   Stewart|         102| 72000| 11/15/2024|\n",
      "|        21|     Justin|      Hill|         104| 71000|   9/5/2023|\n",
      "|        42|      Kayla|    Murphy|         101|  NULL|  6/20/2025|\n",
      "|        19|      Kevin|     White|         102| 68000|  7/25/2023|\n",
      "|        48|      Laura|    Brooks|         103| 81000|       NULL|\n",
      "|        18|     Lauren|      King|         101| 62000|  6/20/2023|\n",
      "|        52|     Lauren|      NULL|         101| 65000|  2/20/2022|\n",
      "|        12|       Mary|    Thomas|         103| 72000| 12/20/2022|\n",
      "|        15|    Matthew|    Wilson|         102| 67000|       NULL|\n",
      "|        22|      Megan|     Scott|         101| 63000| 10/10/2023|\n",
      "|         3|    Michael|   Johnson|         101| 58000|  3/10/2022|\n",
      "|        51|       NULL|    Parker|         105| 60000|  1/15/2022|\n",
      "|        40|    Natalie|      Cook|         103| 79000|  4/10/2025|\n",
      "|        25|   Nicholas|    Morris|         104| 72000|  1/25/2024|\n",
      "|        30|     Olivia|  Campbell|         101| 65000|  6/20/2024|\n",
      "|        41|    Patrick|    Bailey|         104| 76000|  5/15/2025|\n",
      "|        20|     Rachel|    Carter|         103| 74000|  8/30/2023|\n",
      "|        46|    Rebecca|      Ross|         101| 69000| 10/10/2025|\n",
      "|         9|     Robert|    Taylor|         104| 68000|   9/5/2022|\n",
      "|        37|       Ryan|    Fisher|         104| 75000|  1/25/2025|\n",
      "|        36|   Samantha|   Sanchez|         103| 78000| 12/20/2024|\n",
      "|        27|     Samuel|   Roberts|         102| 70000|   3/5/2024|\n",
      "|        44|      Sarah|    Barnes|         103| 80000|  8/30/2025|\n",
      "|         6|      Sarah|  Williams|         101| 59000|  6/18/2022|\n",
      "|        24|    Tiffany|     Perez|         103| 75000| 12/20/2023|\n",
      "|        43|    Timothy|Richardson|         102| 74000|       NULL|\n",
      "|        32|   Victoria|     Evans|         103| 77000|  8/30/2024|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As we can see these employees have different employee ids, we will keep the row with lowest employeeID and remove the remaining rows.\n",
    "\n",
    "# Define a window specification partitioned by FirstName and LastName and ordered by EmployeeID\n",
    "window_spec = Window.partitionBy(\"FirstName\", \"LastName\").orderBy(\"EmployeeID\")\n",
    "\n",
    "# Add a row number column to each row based on the window specification\n",
    "cleaned_employee_df = cleaned_employee_df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Keep only the rows with row_number = 1, which corresponds to the lowest EmployeeID for each unique FirstName and LastName combination\n",
    "deduplicated_employee_df = cleaned_employee_df.filter(\"row_number = 1\").drop(\"row_number\")\n",
    "\n",
    "# Show the deduplicated DataFrame\n",
    "deduplicated_employee_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the cleaned DataFrame as a temporary view\n",
    "deduplicated_employee_df.createOrReplaceTempView(\"deduplicated_employee_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+------+-----------+---------------+\n",
      "|FirstName|LastName|DepartmentID|Salary|JoiningDate|duplicate_count|\n",
      "+---------+--------+------------+------+-----------+---------------+\n",
      "+---------+--------+------------+------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame where FirstName contains 'William' or 'Zachary'\n",
    "duplicates_check_df = spark.sql(\"\"\"\n",
    "    SELECT FirstName, LastName, DepartmentID, Salary, JoiningDate, COUNT(*) AS duplicate_count\n",
    "    FROM deduplicated_employee_view\n",
    "    GROUP BY FirstName, LastName, DepartmentID, Salary, JoiningDate\n",
    "    HAVING COUNT(*) > 1\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicates_check_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "|        39|  William|   Kelly|         102| 73000|   3/5/2025|\n",
      "|        31|  Zachary|  Parker|         102| 71000|  7/25/2024|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the cleaned DataFrame where FirstName contains 'William' or 'Zachary'\n",
    "duplicated_employee_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM deduplicated_employee_view\n",
    "    Where FirstName LIKE '%William%' OR FirstName LIKE '%Zachary%'                                 \n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "duplicated_employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|EmployeeID|  FirstName|  LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "|        13|Christopher| Hernandez|         104|  NULL|  1/25/2023|\n",
      "|        42|      Kayla|    Murphy|         101|  NULL|  6/20/2025|\n",
      "|        48|      Laura|    Brooks|         103| 81000|       NULL|\n",
      "|        52|     Lauren|      NULL|         101| 65000|  2/20/2022|\n",
      "|        15|    Matthew|    Wilson|         102| 67000|       NULL|\n",
      "|        51|       NULL|    Parker|         105| 60000|  1/15/2022|\n",
      "|        43|    Timothy|Richardson|         102| 74000|       NULL|\n",
      "+----------+-----------+----------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for Null values in any of the column\n",
    "null_employee_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM deduplicated_employee_view\n",
    "    Where EmployeeID='NULL' or FirstName ='NULL' or LastName ='NULL' or DepartmentID='NULL' or Salary='NULL' or JoiningDate ='NULL'\n",
    "\"\"\")\n",
    "\n",
    "# Show any remaining duplicates\n",
    "null_employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+------------+------+-----------+\n",
      "|EmployeeID|  FirstName| LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+-----------+---------+------------+------+-----------+\n",
      "|        49|       Adam|   Cooper|         104| 78000|  1/25/2026|\n",
      "|        28|     Alexis|   Turner|         103| 76000|  4/10/2024|\n",
      "|         2|      Alice|    Smith|         102| 65000|  2/20/2022|\n",
      "|        16|     Amanda| Anderson|         103| 73000|  4/10/2023|\n",
      "|        33|     Andrew|  Edwards|         104| 74000|   9/5/2024|\n",
      "|        14|     Ashley|    Lopez|         101| 61000|  2/28/2023|\n",
      "|        29|   Benjamin| Phillips|         104| 73000|  5/15/2024|\n",
      "|        23|    Brandon|    Green|         102| 69000| 11/15/2023|\n",
      "|        17|      Brian|    Moore|         104| 70000|  5/15/2023|\n",
      "|        13|Christopher|Hernandez|         104|  NULL|  1/25/2023|\n",
      "|        11|     Daniel| Anderson|         102| 66000| 11/15/2022|\n",
      "|        38|   Danielle|   Howard|         101| 67000|  2/28/2025|\n",
      "|         5|      David|      Lee|         102| 62000|  5/12/2022|\n",
      "|        26|  Elizabeth|   Rivera|         101| 64000|  2/28/2024|\n",
      "|         4|      Emily|    Brown|         103| 70000|   4/5/2022|\n",
      "|        50|      Emily|   Hughes|         101| 70000|  2/28/2026|\n",
      "|        45|       Eric|    Price|         104| 77000|   9/5/2025|\n",
      "|        34|     Hannah|  Collins|         101| 66000| 10/10/2024|\n",
      "|         7|      James|    Jones|         102| 63000|  7/24/2022|\n",
      "|        47|      Jason|     Long|         102| 75000| 11/15/2025|\n",
      "+----------+-----------+---------+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average salary excluding null values\n",
    "avg_salary = deduplicated_employee_df.select(avg(\"Salary\")).collect()[0][0]\n",
    "\n",
    "# Replace null values in Salary column with average salary\n",
    "deduplicated_employee_df = deduplicated_employee_df.withColumn(\"Salary\", when(col(\"Salary\").isNull(), avg_salary).otherwise(col(\"Salary\")))\n",
    "\n",
    "\n",
    "# Show any remaining duplicates\n",
    "deduplicated_employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+------------+------+-----------+\n",
      "|EmployeeID|FirstName|LastName|DepartmentID|Salary|JoiningDate|\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "+----------+---------+--------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in the Salary column\n",
    "deduplicated_employee_df.filter(col(\"Salary\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- DepartmentID: integer (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- JoiningDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deduplicated_employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+------------+-------+-----------+\n",
      "|EmployeeID|  FirstName| LastName|DepartmentID| Salary|JoiningDate|\n",
      "+----------+-----------+---------+------------+-------+-----------+\n",
      "|        49|       Adam|   Cooper|         104|78000.0|  1/25/2026|\n",
      "|        28|     Alexis|   Turner|         103|76000.0|  4/10/2024|\n",
      "|         2|      Alice|    Smith|         102|65000.0|  2/20/2022|\n",
      "|        16|     Amanda| Anderson|         103|73000.0|  4/10/2023|\n",
      "|        33|     Andrew|  Edwards|         104|74000.0|   9/5/2024|\n",
      "|        14|     Ashley|    Lopez|         101|61000.0|  2/28/2023|\n",
      "|        29|   Benjamin| Phillips|         104|73000.0|  5/15/2024|\n",
      "|        23|    Brandon|    Green|         102|69000.0| 11/15/2023|\n",
      "|        17|      Brian|    Moore|         104|70000.0|  5/15/2023|\n",
      "|        13|Christopher|Hernandez|         104|69680.0|  1/25/2023|\n",
      "|        11|     Daniel| Anderson|         102|66000.0| 11/15/2022|\n",
      "|        38|   Danielle|   Howard|         101|67000.0|  2/28/2025|\n",
      "|         5|      David|      Lee|         102|62000.0|  5/12/2022|\n",
      "|        26|  Elizabeth|   Rivera|         101|64000.0|  2/28/2024|\n",
      "|         4|      Emily|    Brown|         103|70000.0|   4/5/2022|\n",
      "|        50|      Emily|   Hughes|         101|70000.0|  2/28/2026|\n",
      "|        45|       Eric|    Price|         104|77000.0|   9/5/2025|\n",
      "|        34|     Hannah|  Collins|         101|66000.0| 10/10/2024|\n",
      "|         7|      James|    Jones|         102|63000.0|  7/24/2022|\n",
      "|        47|      Jason|     Long|         102|75000.0| 11/15/2025|\n",
      "+----------+-----------+---------+------------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Convert Salary column to IntegerType\n",
    "deduplicated_employee_df = deduplicated_employee_df.withColumn(\"Salary\", deduplicated_employee_df[\"Salary\"].cast(IntegerType()))\n",
    "\n",
    "# Calculate the average salary\n",
    "avg_salary = deduplicated_employee_df.select(avg(\"Salary\")).collect()[0][0]\n",
    "\n",
    "# Replace null values in Salary column with average salary\n",
    "deduplicated_employee_df = deduplicated_employee_df.withColumn(\"Salary\", when(col(\"Salary\").isNull(), avg_salary).otherwise(col(\"Salary\")))\n",
    "\n",
    "# Remove rows where JoiningDate is null\n",
    "deduplicated_employee_df = deduplicated_employee_df.filter(col(\"JoiningDate\").isNotNull())\n",
    "\n",
    "# Show the updated DataFrame\n",
    "deduplicated_employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------+---------+-------+-----------+--------------+---------+\n",
      "|DepartmentID|EmployeeID|  FirstName| LastName| Salary|JoiningDate|DepartmentName|ManagerID|\n",
      "+------------+----------+-----------+---------+-------+-----------+--------------+---------+\n",
      "|         104|        49|       Adam|   Cooper|78000.0|  1/25/2026|            HR|     NULL|\n",
      "|         103|        28|     Alexis|   Turner|76000.0|  4/10/2024|       Finance|        4|\n",
      "|         102|         2|      Alice|    Smith|65000.0|  2/20/2022|     Marketing|        2|\n",
      "|         103|        16|     Amanda| Anderson|73000.0|  4/10/2023|       Finance|        4|\n",
      "|         104|        33|     Andrew|  Edwards|74000.0|   9/5/2024|            HR|     NULL|\n",
      "|         101|        14|     Ashley|    Lopez|61000.0|  2/28/2023|         Sales|        1|\n",
      "|         104|        29|   Benjamin| Phillips|73000.0|  5/15/2024|            HR|     NULL|\n",
      "|         102|        23|    Brandon|    Green|69000.0| 11/15/2023|     Marketing|        2|\n",
      "|         104|        17|      Brian|    Moore|70000.0|  5/15/2023|            HR|     NULL|\n",
      "|         104|        13|Christopher|Hernandez|69680.0|  1/25/2023|            HR|     NULL|\n",
      "|         102|        11|     Daniel| Anderson|66000.0| 11/15/2022|     Marketing|        2|\n",
      "|         101|        38|   Danielle|   Howard|67000.0|  2/28/2025|         Sales|        1|\n",
      "|         102|         5|      David|      Lee|62000.0|  5/12/2022|     Marketing|        2|\n",
      "|         101|        26|  Elizabeth|   Rivera|64000.0|  2/28/2024|         Sales|        1|\n",
      "|         103|         4|      Emily|    Brown|70000.0|   4/5/2022|       Finance|        4|\n",
      "|         101|        50|      Emily|   Hughes|70000.0|  2/28/2026|         Sales|        1|\n",
      "|         104|        45|       Eric|    Price|77000.0|   9/5/2025|            HR|     NULL|\n",
      "|         101|        34|     Hannah|  Collins|66000.0| 10/10/2024|         Sales|        1|\n",
      "|         102|         7|      James|    Jones|63000.0|  7/24/2022|     Marketing|        2|\n",
      "|         102|        47|      Jason|     Long|75000.0| 11/15/2025|     Marketing|        2|\n",
      "+------------+----------+-----------+---------+-------+-----------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the deduplicated_employee_df DataFrame with the department_df DataFrame\n",
    "joined_df = deduplicated_employee_df.join(department_df, on='DepartmentID', how='left')\n",
    "\n",
    "# Show the joined DataFrame\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now, let's do some basic analysis and create some graphs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Salary Distribution by Department**: \n",
    "    - We'll create a boxplot to visualize the distribution of salaries across different departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Boxplot for salary distribution by department\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mboxplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartmentName\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[43mjoined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalary Distribution by Department\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Munna\\Desktop\\D drive\\Study\\VScode\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[1;32mc:\\Users\\Munna\\Desktop\\D drive\\Study\\VScode\\venv\\Lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Boxplot for salary distribution by department\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='DepartmentName', y='Salary', data=joined_df.toPandas())\n",
    "plt.title('Salary Distribution by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Salary')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boxplot visualizes the distribution of salaries across different departments. Each box represents the salary distribution within a specific department. The horizontal line inside each box represents the median salary, while the box itself represents the interquartile range (IQR), with the upper and lower boundaries of the box representing the 75th and 25th percentiles, respectively. The whiskers extend to the minimum and maximum salaries within 1.5 times the IQR from the first and third quartiles, respectively. Any points beyond the whiskers are considered outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Average Salary by Department**: \n",
    "    - We'll calculate the average salary for each department and create a bar plot to visualize it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average salary by department\n",
    "avg_salary_by_department = joined_df.groupBy('DepartmentName').agg(avg('Salary').alias('AverageSalary'))\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "avg_salary_by_department_df = avg_salary_by_department.toPandas()\n",
    "\n",
    "# Bar plot for average salary by department\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='DepartmentName', y='AverageSalary', data=avg_salary_by_department_df)\n",
    "plt.title('Average Salary by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar plot displays the average salary for each department. Each bar represents a department, and the height of the bar corresponds to the average salary of employees within that department. It provides a comparison of the average salary levels across different departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Employee Count by Department**: \n",
    "    - We'll count the number of employees in each department and create a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count employee by department\n",
    "employee_count_by_department = joined_df.groupBy('DepartmentName').count()\n",
    "\n",
    "# Bar plot for employee count by department\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='DepartmentName', y='count', data=employee_count_by_department.toPandas())\n",
    "plt.title('Employee Count by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Number of Employees')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar plot illustrates the number of employees in each department. Each bar represents a department, and the height of the bar indicates the total count of employees within that department. It helps to understand the distribution of workforce among different departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Employee Count by Manager**: \n",
    "    - We'll count the number of employees managed by each manager and create a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count employee by manager\n",
    "employee_count_by_manager = joined_df.groupBy('ManagerID').count()\n",
    "\n",
    "# Convert ManagerID to integers\n",
    "employee_count_by_manager = employee_count_by_manager.withColumn(\"ManagerID\", employee_count_by_manager[\"ManagerID\"].cast(IntegerType()))\n",
    "\n",
    "# Bar plot for employee count by manager\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='ManagerID', y='count', data=employee_count_by_manager.toPandas())\n",
    "plt.title('Employee Count by Manager')\n",
    "plt.xlabel('Manager ID')\n",
    "plt.ylabel('Number of Employees')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar plot shows the number of employees managed by each manager. Each bar represents a manager (identified by ManagerID), and the height of the bar represents the count of employees under their management. It provides insights into the workload distribution among managers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary Distribution:\n",
    "We'll create a histogram to visualize the distribution of salaries across all employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for salary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=joined_df.toPandas(), x='Salary', bins=20, kde=True)\n",
    "plt.title('Salary Distribution')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram displays the distribution of salaries across all employees. The x-axis represents salary ranges, while the y-axis represents the frequency of employees within each salary range. It provides an overview of the salary distribution pattern within the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employee Count Over Time:\n",
    "We'll plot the number of employees joining over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JoiningDate column to DateType\n",
    "joined_df = joined_df.withColumn(\"JoiningDate\", to_date(col(\"JoiningDate\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "# Count employee by joining date\n",
    "employee_count_over_time = joined_df.groupBy('JoiningDate').count().orderBy('JoiningDate')\n",
    "\n",
    "# Line plot for employee count over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='JoiningDate', y='count', data=employee_count_over_time.toPandas())\n",
    "plt.title('Employee Count Over Time')\n",
    "plt.xlabel('Joining Date')\n",
    "plt.ylabel('Number of Employees')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line plot depicts the trend of employee count over time based on their joining dates. The x-axis represents the joining dates, while the y-axis represents the number of employees who joined on each date. It helps to visualize the hiring trend and understand how the workforce has evolved over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
